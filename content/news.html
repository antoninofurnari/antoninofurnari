<ul>
<li><b>July 2024</b>: Three papers accepted at <a href="http://eccv2024.ecva.net">ECCV 2024!</a>
    <ul>
        <li>Lorenzo Mur-Labadia, Ruben Martinez-Cantin, Josechu Guerrero, Giovanni Maria Farinella, Antonino Furnari. AFF-ttention! Affordances and Attention models for Short-Term Object Interaction Anticipation. <a target="_blank" href="https://arxiv.org/pdf/2406.01194">[Paper]</a></li>
        <li>Camillo Quattrocchi, Antonino Furnari, Daniele Di Mauro, Mario Valerio Giuffrida, Giovanni Maria Farinella. Synchronization is All You Need: Exocentric-to-Egocentric Transfer for Temporal Action Segmentation with Unlabeled Synchronized Video Pairs. <a target="_blank" href="https://arxiv.org/pdf/2312.02638">[Paper]</a></li>
        <li>Rosario Leonardi, Antonino Furnari, Francesco Ragusa, Giovanni Maria Farinella. Are Synthetic Data Useful for Egocentric Hand-Object Interaction Detection? <a target="_blank" href="https://arxiv.org/pdf/2312.02672">[Paper]</a></li>
    </ul>
</li>
<li><b>May 2024</b>: Serving as an Area Chair for <a href="https://neurips.cc">NeurIPS 2024</a> - Benchmarks and Datasets Track</li>
<li><b>June 2024</b>: Serving as an Area Chair for <a href="https://wacv2025.thecvf.com">WACV 2025</a></li>
<li><b>June 2024</b>: Organizing the <a href="https://egovis.github.io/cvpr24/">First Joint Workshop on Egocentric Vision (EgoVis) in conjunction with CVPR 2024</a></li>
<li><b>April 2024</b>: Giving a seminar at the University of Zaragoza <a href="https://sites.google.com/unizar.es/rseminars-mrgcv" target="_blank">Research Seminars course</a></li>
<li><b>March 2024</b>: Three papers accepted at <a href="http://cvpr.thecvf.com">CVPR 2024</a>! (1 oral + 2 posters)
<ul>
    <li>Alessandro Flaborea, Guido Maria D'Amely di Melendugno, Leonardo Plini, Luca Scofano, Edoardo De Matteis, Antonino Furnari, Giovanni Maria Farinella, Fabio Galasso. PREGO: online mistake detection in PRocedural EGOcentric videos<a href="https://arxiv.org/pdf/2404.01933" target="_blank">[Paper]</a></li>
    <li>Ivan Rodin, Antonino Furnari, Kyle Min, Subarna Tripathi, Giovanni Maria Farinella. Action Scene Graphs for Long-Form Understanding of Egocentric Videos. <a href="https://arxiv.org/pdf/2312.03391" target="_blank">[Paper]</a></li>
    <li>Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives. With other 100 authors! <b>Oral &lt; 1% accept rate.</b> <a href="https://arxiv.org/pdf/2311.18259" target="_blank">[Paper]</a></li>
</ul>
</li>
<li><b>March 2024</b>: I'll serve as Program Chair of the <a href="http://visapp.scitevents.org">VISAPP 2025</a> Conference</li>
<li><b>November 2023</b>: The <a href="https://sites.google.com/view/prin-pnrr-team">TEAM</a> project has officially started!</li>
<li><b>September 2023</b>: The <a href="https://sites.google.com/view/extraeye">EXTRA-EYE</a> project has officially started!</li>
<li><b>September 2023</b>: Giving a <a href="https://www.antoninofurnari.it/talks/iciap2023/">tutorial on Egocentric Vision</a> at <a href="">ICIAP 2023</a></li>
<li><b>September 2023</b>: Winners of the <a href="https://iplab.dmi.unict.it/MECCANO/challenge.html">2023 MECCANO Challenge</a> Announced</li>
<li><b>August 2023</b>: Serving as an Area Chair for <a href="https://wacv2024.thecvf.com/">WACV 2024</a></li>
<li><b>August 2023</b>: Aug 2023: Survey paper <a href="https://openreview.net/forum?id=V3974SUk1w">An Outlook into the Future of Egocentric Vision</a> open for comments on OpenReview until 15 Sep.</li>
<li><b>July 2023</b>: PNRR PRIN Project "TEAM" has been accepted and will be funded by the Italian ministry of University and Research</li>
<li><b>July 2023</b>: Serving as Academic Assessment at <a href="http://iplab.dmi.unict.it/icvss2023/">ICVSS 2023</a></li>
<li><b>June 2023</b>: PRIN Project "EXTRA-EYE" has been accepted and will be funded by the Italian ministry of University and Research</li>
<li><b>June 2023</b>: Paper "<a href="https://www.sciencedirect.com/science/article/pii/S1077314223001431">Streaming egocentric action anticipation: an evaluation scheme and approach</a>" accepted for publication in the Computer Vision and Image Understanding Journal</li>
<li><b>June 2023</b>: Paper "<a href="https://www.sciencedirect.com/science/article/pii/S1077314223001443">MECCANO: A multimodal egocentric dataset for humans behavior understanding in the industrial-like domain</a> accepted for publication in the Computer Vision and Image Understanding Journal</li>
<li><b>June 2023</b>: Winners of the <a href="https://epic-kitchens.github.io/2023#results">2023 EPIC-KITCHENS Challenges</a> Announced</li>
<li><b>June 2023</b>: Winners of the <a href="https://epic-kitchens.github.io/2023#results">2023 EPIC-KITCHENS Challenges</a> Announced</li>
<li><b>June 2023</b>: Winners of the <a href="https://ego4d-data.org/workshops/cvpr23/">2023 EGO4D Challenges Announced</a></li>
<li><b>March 2023</b>: I'll serve as Program Chair of the <a href="http://visapp.scitevents.org">VISAPP 2024</a> Conference</li>
<li><b>March 2023</b>: I'll be giving a tutorial at the Italian Summer School <a href="http://vismac23.github.io">VISMAC 2023</a></li>
<li><b>January 2023</b>: I am now an <a href="https://ellis.eu/members">ELLIS</a> member</a></li>
<li><b>November 2022</b>: I'll be an Area Chair for ICCV 2023</li>
<li><b>March 2022</b>: The <a href="https://ego4d-data.org">EGO4D</a> paper is accepted for presentation at CVPR 2023</li>
<li><b>February 2022</b>: The <a href="https://ego4d-data.org">EGO4D</a> dataset is publicly available</li>
</ul>