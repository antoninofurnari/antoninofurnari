---
title: "Action Scene Graphs for Long-Form Understanding of Egocentric Videos"
date: 2024-01-03
draft: false
bibtex: ["@inproceedings{rodin2023action,
  primaryclass = { cs.CV },
  archiveprefix = { arXiv },
  eprint = { 2312.03391 },
  pdf = { https://arxiv.org/pdf/2312.03391.pdf },
  year = { 2024 },
  booktitle = {  Conference on Computer Vision and Pattern Recognition (CVPR)  },
  title = { Action Scene Graphs for Long-Form Understanding of Egocentric Videos },
  author = { Ivan Rodin and Antonino Furnari and Kyle Min and Subarna Tripathi and Giovanni Maria Farinella },
  url = {https://github.com/fpv-iplab/EASG}
}"]
teaser: easg.jpg
---

We present Egocentric Action Scene Graphs (EASGs), a new representation for long-form understanding of egocentric videos. EASGs extend standard manually-annotated representations of egocentric videos, such as verb-noun action labels, by providing a temporally evolving graphbased description of the actions performed by the camera wearer, including interacted objects, their relationships, and how actions unfold in time. Through a novel annotation procedure, we extend the Ego4D dataset by adding manually labeled Egocentric Action Scene Graphs offering a rich set of annotations designed for long-from egocentric video understanding. We hence define the EASG generation task and provide a baseline approach, establishing preliminary benchmarks. Experiments on two downstream tasks, egocentric action anticipation and egocentric activity summarization, highlight the effectiveness of EASGs for long-form egocentric video understanding.
<a href="https://github.com/fpv-iplab/EASG">Web Page</a>